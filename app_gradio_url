import os
import torch
import gradio as gr

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# --------------------------
# Config
# --------------------------
BASE_MODEL_ID = "meta-llama/Llama-3.2-1B-Instruct"

# override this with an env var if desired
ADAPTER_PATH = os.environ.get("ADAPTER_PATH", "phishsense_lora_adapter_url")

HF_TOKEN = os.environ.get("HF_TOKEN", None)
if HF_TOKEN is None:
    raise ValueError("Please set HF_TOKEN in your environment (Hugging Face access token).")

PROMPT_PREFIX = (
    "Classify the following text as phishing or not. "
    "Respond with 'TRUE' or 'FALSE':\n\n"
)

# --------------------------
# Load model + adapter
# --------------------------
print(f"Loading base model: {BASE_MODEL_ID}")
base_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL_ID,
    token=HF_TOKEN,
    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
    device_map="auto",
)

print(f"Loading LoRA adapter from: {ADAPTER_PATH}")
model = PeftModel.from_pretrained(
    base_model,
    ADAPTER_PATH,
)

tokenizer = AutoTokenizer.from_pretrained(ADAPTER_PATH or BASE_MODEL_ID, token=HF_TOKEN)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

device = next(model.parameters()).device
print("Model loaded on:", device)

# Prediction function

def classify_text(user_input: str) -> str:
    """
    Takes raw input (URL or email text), returns 'TRUE' or 'FALSE'
    indicating phishing vs. not phishing.
    """
    if not user_input.strip():
        return "Please enter some text or a URL."

    prompt = f"{PROMPT_PREFIX}{user_input}\nAnswer:"

    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        truncation=True,
        max_length=512,
    ).to(device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=4,
            do_sample=False,
            temperature=0.0,
            pad_token_id=tokenizer.eos_token_id,
        )

    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # Just look at the part after "Answer:"
    answer_part = generated.split("Answer:")[-1].strip().upper()

    if "TRUE" in answer_part:
        return "TRUE (phishing)"
    if "FALSE" in answer_part:
        return "FALSE (not phishing)"

    # Fallback if parsing is weird
    return f"Could not parse model output: {answer_part}"



# Gradio UI

demo = gr.Interface(
    fn=classify_text,
    inputs=gr.Textbox(
        lines=4,
        placeholder="Paste an email OR a URL here...",
        label="Email or URL to classify",
    ),
    outputs=gr.Textbox(label="Prediction (TRUE = phishing, FALSE = not phishing)"),
    title="PhishProof â€“ URL Fine-tuned LLaMA Classifier",
    description=(
        "This demo uses Llama-3.2-1B-Instruct fine-tuned with LoRA on a URL phishing dataset. "
        "Paste a URL (or email text) and the model will classify it as phishing (TRUE) or not (FALSE)."
    ),
)

if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7860)
